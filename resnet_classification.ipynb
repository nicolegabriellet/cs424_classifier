{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bc07019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46268c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# OUTPUT_DIR = \"./smu_no_logo_jpg/\"\n",
    "# img_data = {}\n",
    "\n",
    "# for folder in os.listdir(OUTPUT_DIR):\n",
    "#     subfolder_path = os.path.join(OUTPUT_DIR, folder)\n",
    "#     if os.path.isdir(subfolder_path):  # Check if it's a directory\n",
    "#         img_data[folder] = {}\n",
    "#         for subfolder in os.listdir(subfolder_path):\n",
    "#             subfolder_full_path = os.path.join(subfolder_path, subfolder)\n",
    "#             if os.path.isdir(subfolder_full_path):  # Check if it's a directory\n",
    "#                 img_data[folder][subfolder] = len(os.listdir(subfolder_full_path))\n",
    "\n",
    "# df = pd.DataFrame.from_dict(img_data, orient='index')\n",
    "# df.T.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "003c9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "# Assume your data is organized in this structure:\n",
    "# smu_no_logo_jpg/\n",
    "#    class1/\n",
    "#        img1.jpg\n",
    "#        img2.jpg\n",
    "#        ...\n",
    "#    class2/\n",
    "#        img1.jpg\n",
    "#        img2.jpg\n",
    "#        ...\n",
    "\n",
    "OUTPUT_DIR = \"./smu_no_logo_jpg/\"\n",
    "\n",
    "# Image transformations\n",
    "im_size = 224\n",
    "transformation = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(im_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load the dataset from the directory\n",
    "full_dataset = datasets.ImageFolder(OUTPUT_DIR, transform=transformation)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "# Define your split sizes here\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - (train_size + val_size)\n",
    "\n",
    "# Use random_split for splitting the dataset\n",
    "train_data, val_data, test_data = random_split(full_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Creating data loaders for each split\n",
    "BATCH_SIZE = 20\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Now you can use train_loader, val_loader, and test_loader for your training, validation, and testing loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "944ab126",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.RandomVerticalFlip(0.5),\n",
    "        transforms.RandomGrayscale(0.5),\n",
    "        transforms.RandomAdjustSharpness(0.5),\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4),\n",
    "        transforms.GaussianBlur(3),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a693d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x28bc17210>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bad2793b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d0cd298",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_num_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder, path_list \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path_list \u001b[38;5;241m<\u001b[39m max_num_img:\n\u001b[1;32m      3\u001b[0m         sets \u001b[38;5;241m=\u001b[39m (max_num_img\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(path_list)) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(path_list)\n\u001b[1;32m      4\u001b[0m         mod \u001b[38;5;241m=\u001b[39m (max_num_img\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(path_list)) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(path_list)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_num_img' is not defined"
     ]
    }
   ],
   "source": [
    "for folder, path_list in train_data:\n",
    "    if path_list < max_num_img:\n",
    "        sets = (max_num_img-len(path_list)) // len(path_list)\n",
    "        mod = (max_num_img-len(path_list)) % len(path_list)\n",
    "        for i, path in enumerate(path_list):\n",
    "            img = Image.open(path)\n",
    "            img = img.convert('RGB')\n",
    "            sets_iter = sets + 1 if i < mod else sets\n",
    "            for k in range(sets_iter):\n",
    "                save_image(image_transforms(img), f'{OUTPUT_DIR}/train/{folder}/augmented_{i}_{k}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd5d049d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './smu_no_logo_jpg/train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m OUTPUT_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./smu_no_logo_jpg/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(OUTPUT_DIR\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m OUTPUT_DIR\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m     train_data[folder] \u001b[38;5;241m=\u001b[39m [img_path \u001b[38;5;241m+\u001b[39m img \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(img_path)]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './smu_no_logo_jpg/train/'"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "\n",
    "for folder in os.listdir(OUTPUT_DIR+'train/'):\n",
    "    img_path = OUTPUT_DIR+'train/' + folder + '/'\n",
    "    train_data[folder] = [img_path + img for img in os.listdir(img_path)]\n",
    "\n",
    "max_num_img = max([len(v) for k,v in train_data.items()])\n",
    "print(f\"Number of images in largest class: {max_num_img}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7d71b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader):\n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for images, labels in iter(val_loader):\n",
    "        output = model.forward(images)\n",
    "        val_loss += criterion(output, labels).item()\n",
    "\n",
    "        probabilities = torch.exp(output)\n",
    "\n",
    "        equality = labels.data == probabilities.max(dim=1)[1]\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de04a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_loader, val_loader, epochs=1):\n",
    "    plot_training = []\n",
    "    plot_validation = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for images, labels in iter(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Evaluate performance of each epoch\n",
    "        model.eval()\n",
    "\n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            validation_loss, accuracy = validation(model, criterion, val_loader)\n",
    "\n",
    "        print(\n",
    "            \"Epoch: {}/{}.. \".format(e + 1, epochs),\n",
    "            \"Training Loss: {:.3f}.. \".format(running_loss),\n",
    "            \"Validation Loss: {:.3f}.. \".format(\n",
    "                validation_loss / len(val_loader)\n",
    "            ),\n",
    "            \"Validation Accuracy: {:.3f}\".format(accuracy / len(val_loader)),\n",
    "        )\n",
    "\n",
    "        plot_training.append(running_loss)\n",
    "        plot_validation.append(validation_loss / len(val_loader))\n",
    "    \n",
    "    plt.plot(range(len(plot_training)), plot_training, label='training')\n",
    "    plt.plot(range(len(plot_validation)), plot_validation, label='validation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32ae35d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, class_mapping, arch):\n",
    "    \"\"\"\n",
    "    Save trained model weights.\n",
    "\n",
    "    Input:\n",
    "    arch(str): Model architecture\n",
    "    \"\"\"\n",
    "\n",
    "    checkpoint = {\n",
    "        \"arch\": arch,\n",
    "        \"class_to_idx\": class_mapping,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "    }\n",
    "\n",
    "    timestamp = datetime.now().date().strftime(\"%Y%m%d\")\n",
    "    torch.save(checkpoint, f\"./checkpoint/{timestamp}_{arch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f521f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming full_dataset is your ImageFolder dataset\n",
    "class_mapping = full_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "86dc4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(weights=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2817386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained model parameters to avoid backpropogating through them\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33749c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original final layer\n",
      "Linear(in_features=2048, out_features=1000, bias=True)\n",
      "\n",
      "Modified final layer\n",
      "Sequential(\n",
      "  (fc): Linear(in_features=2048, out_features=13, bias=True)\n",
      "  (output): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Original final layer\")\n",
    "print(model.fc)\n",
    "\n",
    "# Build custom classifier\n",
    "num_classes = 13\n",
    "num_ftrs = model.fc.in_features\n",
    "classifier = nn.Sequential(\n",
    "    OrderedDict(\n",
    "        [\n",
    "            (\"fc\", nn.Linear(num_ftrs, num_classes)),\n",
    "            (\"output\", nn.LogSoftmax(dim=1)),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "model.fc = classifier\n",
    "\n",
    "print(\"\\nModified final layer\")\n",
    "print(model.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71f974c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.01, 1.0, 0.001)\n",
    "y = -np.log(x)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(x,y)\n",
    "plt.ylabel('-log(x)')\n",
    "plt.xlabel('x')\n",
    "plt.title('Negative log-likelihood range')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a37385c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and gradient descent\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ba3076b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m resnet_model \u001b[38;5;241m=\u001b[39m train_model(model, optimizer, criterion, train_loader, val_loader)\n",
      "Cell \u001b[0;32mIn[58], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(train_loader):\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(images)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:152\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "resnet_model = train_model(model, optimizer, criterion, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dce1c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(resnet_model, class_mapping, 'resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f718cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4786ca9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'CIS', 1: 'SOL', 2: 'admin_bldg', 3: 'concourse', 4: 'connex', 5: 'exterior', 6: 'fishtanks', 7: 'inside_sr', 8: 'interior', 9: 'kgc', 10: 'koufu', 11: 'lks', 12: 'non_smu'}\n"
     ]
    }
   ],
   "source": [
    "class_mapping = {value: key for key, value in class_mapping.items()}\n",
    "\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36d3da0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: lks with confidence 0.94\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model\n",
    "# model = torch.load('./checkpoint/20240320_resnet50.pt')\n",
    "checkpoint = torch.load('./checkpoint/20240320_resnet50.pt')\n",
    "resnet_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "resnet_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define the same transformations as used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load and transform the image\n",
    "img = Image.open('/Users/nicoletan/Documents/SMU/Y2S2/CS424/project/classifier/smu_no_logo_jpg/lks/img_3854.jpg').convert('RGB')\n",
    "img_t = transform(img)\n",
    "batch_t = img_t.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():  # No need to track gradients\n",
    "    output = resnet_model(batch_t)\n",
    "\n",
    "# Assuming the model outputs raw scores, which are often the case for classification models\n",
    "predicted_probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "predicted_index = predicted_probabilities.argmax(1).item()\n",
    "\n",
    "# Load your class mapping\n",
    "# class_mapping = {0: 'class_name_1', 1: 'class_name_2', ...}  # Update this based on your classes\n",
    "\n",
    "# Map the predicted index to a class\n",
    "predicted_class = class_mapping[predicted_index]\n",
    "print(f'Predicted class: {predicted_class} with confidence {predicted_probabilities.max().item():.2f}')\n",
    "if predicted_class != \"non_smu\":\n",
    "    print(\"smu\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
